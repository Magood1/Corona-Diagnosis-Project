{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMIvWK9dYyZh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-E6zPj7ZdFH"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KAGGLE_USERNAME\"] = \"magdhndi\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"9b6c8953fb75d807a407f863ae22edc6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvFKYzhJZgiu",
    "outputId": "24fa0553-8a78-469c-8491-3ce21d446d0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\n",
      "License(s): copyright-authors\n",
      "Downloading covid19-radiography-database.zip to /content\n",
      " 98% 764M/778M [00:08<00:00, 88.6MB/s]\n",
      "100% 778M/778M [00:08<00:00, 101MB/s] \n",
      "Dataset URL: https://www.kaggle.com/datasets/artyomkolas/3-kinds-of-pneumonia\n",
      "License(s): Attribution 4.0 International (CC BY 4.0)\n",
      "Downloading 3-kinds-of-pneumonia.zip to /content\n",
      "100% 3.48G/3.49G [00:41<00:00, 86.6MB/s]\n",
      "100% 3.49G/3.49G [00:41<00:00, 90.0MB/s]\n",
      "Dataset URL: https://www.kaggle.com/datasets/darshan1504/covid19-detection-xray-dataset\n",
      "License(s): Attribution 4.0 International (CC BY 4.0)\n",
      "Downloading covid19-detection-xray-dataset.zip to /content\n",
      " 95% 177M/186M [00:02<00:00, 60.0MB/s]\n",
      "100% 186M/186M [00:02<00:00, 77.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download tawsifurrahman/covid19-radiography-database\n",
    "!kaggle datasets download artyomkolas/3-kinds-of-pneumonia\n",
    "!kaggle datasets download darshan1504/covid19-detection-xray-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frlQYV8tZhUW"
   },
   "outputs": [],
   "source": [
    "!unzip covid19-radiography-database\n",
    "!unzip 3-kinds-of-pneumonia\n",
    "!unzip covid19-detection-xray-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAAwCaIiZpgQ"
   },
   "outputs": [],
   "source": [
    "normal_paths = ['COVID-19_Radiography_Dataset/Normal/images', 'Curated X-Ray Dataset/Normal', 'NonAugmentedTrain/Normal']\n",
    "covid_paths = ['COVID-19_Radiography_Dataset/COVID/images', 'Curated X-Ray Dataset/COVID-19', 'NonAugmentedTrain/COVID-19']\n",
    "viralPneumonia_paths = ['COVID-19_Radiography_Dataset/Viral Pneumonia/images', 'Curated X-Ray Dataset/Pneumonia-Viral', 'NonAugmentedTrain/ViralPneumonia']\n",
    "Lung_Opacity_paths = ['COVID-19_Radiography_Dataset/Lung_Opacity/images']\n",
    "BacterialPneumonia_paths = ['Curated X-Ray Dataset/Pneumonia-Bacterial', 'NonAugmentedTrain/BacterialPneumonia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzEjiOjKZr3M"
   },
   "outputs": [],
   "source": [
    "all_paths = [normal_paths, covid_paths, viralPneumonia_paths, BacterialPneumonia_paths, Lung_Opacity_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUlLRWAeZvd9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def load_images(paths, target_label, target_size=(100, 100), Max=3413):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for folder_path in paths:\n",
    "        images_names = os.listdir(folder_path)\n",
    "        for image_name in images_names:\n",
    "          if len(images) >= Max:\n",
    "            break\n",
    "          image_path = os.path.join(folder_path, image_name)\n",
    "          image = load_img(image_path, target_size=target_size)\n",
    "          image_array = img_to_array(image) / 255.0  # Normalize the image\n",
    "          images.append(image_array)\n",
    "          labels.append(target_label)  # Use the corresponding label from 'target' list\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKaH7HVLZwqP"
   },
   "outputs": [],
   "source": [
    "# Preprocess image data\n",
    "data_images = []\n",
    "data_targets = []\n",
    "for target_label, db_images in enumerate(all_paths):\n",
    "  images, target = load_images(db_images, target_label)\n",
    "  data_images.extend(images)\n",
    "  data_targets.extend(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIc3GeeyZ2Tg"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def encode_labels(labels):\n",
    "    \"\"\"\n",
    "    Encode categorical labels into a numerical format using one-hot encoding.\n",
    "\n",
    "    Args:\n",
    "        labels (np.array): Array of categorical labels.\n",
    "\n",
    "    Returns:\n",
    "        np.array: One-hot encoded labels.\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(labels)\n",
    "    one_hot_encoded = to_categorical(integer_encoded)\n",
    "\n",
    "    return one_hot_encoded, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwMaNWyBZ2nf"
   },
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "labels_one_hot, label_encoder = encode_labels(data_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2w1XyVPZ2zt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(images, labels, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        images (np.array): Array of images.\n",
    "        labels (np.array): Array of labels.\n",
    "        test_size (float): Proportion of the data to include in the test split.\n",
    "        val_size (float): Proportion of the data to include in the validation split.\n",
    "        random_state (int): Seed used by the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Split data (X_train, X_val, X_test, y_train, y_val, y_test).\n",
    "    \"\"\"\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=(test_size + val_size), random_state=random_state)\n",
    "    val_ratio = val_size / (test_size + val_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_ratio, random_state=random_state)\n",
    "\n",
    "    return np.asarray(X_train), np.asarray(X_val), np.asarray(X_test), np.asarray(y_train), np.asarray(y_val), np.asarray(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPv5moeuZ8us"
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(data_images, labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3tEtrgAZ-no"
   },
   "outputs": [],
   "source": [
    "data_images, labels_one_hot = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "965pLR6daAne"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    \"\"\"\n",
    "    Shuffle the data to ensure randomness.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Array of images.\n",
    "        y (np.array): Array of labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Shuffled data (X_shuffled, y_shuffled).\n",
    "    \"\"\"\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO49e4SdaC5e"
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "X_train, y_train = shuffle_data(X_train, y_train)\n",
    "X_val, y_val = shuffle_data(X_val, y_val)\n",
    "X_test, y_test = shuffle_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvgyuIh-aFMn",
    "outputId": "e4c0e596-bfe1-4cec-f52c-ebfe86553377"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11945, 11945)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTv7DwfbaHJa",
    "outputId": "430dd2b9-6fa1-4cfe-d595-9e3f4deeaee8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3413, 3413)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDSFnFruaJoF",
    "outputId": "947422e9-d204-459a-cbcb-6b8ad1a5dc82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1707, 1707)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7vWSairaJj5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def prepare_batches(X, y, batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepare data batches for training.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Array of images.\n",
    "        y (np.array): Array of labels.\n",
    "        batch_size (int): Size of the batches.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Batched dataset.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKFz0SceaSfg"
   },
   "outputs": [],
   "source": [
    "# Prepare batches\n",
    "train_dataset = prepare_batches(X_train, y_train)\n",
    "val_dataset = prepare_batches(X_val, y_val)\n",
    "test_dataset = prepare_batches(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePJm1dsbaTKZ"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = 0,0,0, 0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hl20WbeJaVzh"
   },
   "outputs": [],
   "source": [
    "def cache_and_prefetch(dataset, cache=True, buffer_size=tf.data.AUTOTUNE):\n",
    "    \"\"\"\n",
    "    Cache and prefetch the dataset for performance optimization.\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): Input dataset.\n",
    "        cache (bool): Whether to cache the dataset.\n",
    "        buffer_size (int): Buffer size for prefetching.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Optimized dataset.\n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cQ8sqxLaX6d"
   },
   "outputs": [],
   "source": [
    "# Cache and prefetch\n",
    "train_dataset = cache_and_prefetch(train_dataset)\n",
    "val_dataset = cache_and_prefetch(val_dataset)\n",
    "test_dataset = cache_and_prefetch(test_dataset, cache=False)  # Typically don't cache test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D95EyM2haZnV",
    "outputId": "0876af4d-e52d-4afa-bf0c-5708e8fef3a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUPYx9zeacEJ",
    "outputId": "3bd975e3-280d-4b99-b6b1-d096a8b4bcd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the CNN model with regularization\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(5, activation='softmax')(x)\n",
    "\n",
    "image_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "image_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jd8mkVioagNR"
   },
   "outputs": [],
   "source": [
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PanbQkGkb-et"
   },
   "outputs": [],
   "source": [
    "# Callbacks for early stopping and saving the best model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-9qRGWTb_sg"
   },
   "outputs": [],
   "source": [
    "# Class weights to balance the contributions of each class\n",
    "class_weights = {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZf4laxkcDlD",
    "outputId": "c955c8f3-b864-4025-e3dc-a2507e66ebba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "374/374 [==============================] - ETA: 0s - loss: 6.0677 - accuracy: 0.7378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r374/374 [==============================] - 86s 124ms/step - loss: 6.0677 - accuracy: 0.7378 - val_loss: 2.0516 - val_accuracy: 0.2543\n",
      "Epoch 2/10\n",
      "374/374 [==============================] - 42s 112ms/step - loss: 0.7224 - accuracy: 0.7890 - val_loss: 1.3593 - val_accuracy: 0.4216\n",
      "Epoch 3/10\n",
      "374/374 [==============================] - 44s 118ms/step - loss: 0.6224 - accuracy: 0.8148 - val_loss: 1.1734 - val_accuracy: 0.5640\n",
      "Epoch 4/10\n",
      "374/374 [==============================] - 46s 124ms/step - loss: 0.5701 - accuracy: 0.8366 - val_loss: 1.0261 - val_accuracy: 0.5734\n",
      "Epoch 5/10\n",
      "374/374 [==============================] - 40s 108ms/step - loss: 0.5622 - accuracy: 0.8499 - val_loss: 1.1039 - val_accuracy: 0.6408\n",
      "Epoch 6/10\n",
      "374/374 [==============================] - 41s 109ms/step - loss: 0.5071 - accuracy: 0.8669 - val_loss: 1.7513 - val_accuracy: 0.4629\n",
      "Epoch 7/10\n",
      "374/374 [==============================] - 49s 130ms/step - loss: 0.4203 - accuracy: 0.8891 - val_loss: 0.7140 - val_accuracy: 0.7609\n",
      "Epoch 8/10\n",
      "374/374 [==============================] - 47s 127ms/step - loss: 0.3570 - accuracy: 0.9087 - val_loss: 0.6291 - val_accuracy: 0.7890\n",
      "Epoch 9/10\n",
      "374/374 [==============================] - 43s 115ms/step - loss: 0.3231 - accuracy: 0.9198 - val_loss: 0.5916 - val_accuracy: 0.8362\n",
      "Epoch 10/10\n",
      "374/374 [==============================] - 41s 109ms/step - loss: 0.2984 - accuracy: 0.9231 - val_loss: 0.8081 - val_accuracy: 0.7422\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs for initial training\n",
    "initial_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "history = image_model.fit(train_dataset,\n",
    "                          validation_data=val_dataset,\n",
    "                          epochs=initial_epochs,\n",
    "                          class_weight=class_weights,\n",
    "                          callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwxPFEXwcGDF"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the top layers of the base model\n",
    "for layer in base_model.layers[-20:]:  # Adjusted number of layers to unfreeze\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veqoBdjwgpNX"
   },
   "outputs": [],
   "source": [
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "image_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYB_kIEUgslW",
    "outputId": "5c71a107-96ca-41fd-faa4-ddefc03be8d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "374/374 [==============================] - 27s 53ms/step - loss: 0.4670 - accuracy: 0.8625 - val_loss: 0.5382 - val_accuracy: 0.8213\n",
      "Epoch 2/20\n",
      "374/374 [==============================] - 19s 50ms/step - loss: 0.3667 - accuracy: 0.8872 - val_loss: 0.4889 - val_accuracy: 0.8330\n",
      "Epoch 3/20\n",
      "374/374 [==============================] - 19s 51ms/step - loss: 0.3267 - accuracy: 0.8965 - val_loss: 0.4650 - val_accuracy: 0.8412\n",
      "Epoch 4/20\n",
      "374/374 [==============================] - 19s 51ms/step - loss: 0.2931 - accuracy: 0.9066 - val_loss: 0.4534 - val_accuracy: 0.8476\n",
      "Epoch 5/20\n",
      "374/374 [==============================] - 19s 51ms/step - loss: 0.2583 - accuracy: 0.9206 - val_loss: 0.4470 - val_accuracy: 0.8573\n",
      "Epoch 6/20\n",
      "374/374 [==============================] - 18s 49ms/step - loss: 0.2257 - accuracy: 0.9321 - val_loss: 0.4472 - val_accuracy: 0.8640\n",
      "Epoch 7/20\n",
      "374/374 [==============================] - 18s 49ms/step - loss: 0.1965 - accuracy: 0.9416 - val_loss: 0.4512 - val_accuracy: 0.8690\n",
      "Epoch 8/20\n",
      "374/374 [==============================] - 18s 49ms/step - loss: 0.1686 - accuracy: 0.9536 - val_loss: 0.4584 - val_accuracy: 0.8723\n",
      "Epoch 9/20\n",
      "374/374 [==============================] - 18s 49ms/step - loss: 0.1402 - accuracy: 0.9640 - val_loss: 0.4681 - val_accuracy: 0.8731\n",
      "Epoch 10/20\n",
      "374/374 [==============================] - 19s 49ms/step - loss: 0.1122 - accuracy: 0.9746 - val_loss: 0.4805 - val_accuracy: 0.8740\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs for fine-tuning\n",
    "fine_tuning_epochs = 20  # Adjusted based on expected complexity\n",
    "\n",
    "# Continue training the model\n",
    "fine_tuning_history = image_model.fit(train_dataset,\n",
    "                                      validation_data=val_dataset,\n",
    "                                      epochs=fine_tuning_epochs,\n",
    "                                      class_weight=class_weights,\n",
    "                                      callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjkJdWqqg3pH",
    "outputId": "434c473a-7bc0-40f0-b843-09782d7d0147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 3s 44ms/step - loss: 0.4410 - accuracy: 0.8612\n",
      "Test accuracy: 86.12%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = image_model.evaluate(test_dataset)\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECGBw2Gph-5F"
   },
   "outputs": [],
   "source": [
    "image_model.save('/content/drive/My Drive/path_to_save/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSx2AXOyiK3Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def TFNP(cm):\n",
    "    \"\"\"\n",
    "    Calculate True Positives, False Negatives, False Positives, and True Negatives for each class.\n",
    "    \"\"\"\n",
    "    all_samples = np.sum(cm)\n",
    "    cm_classes = []\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        tp = cm[i, i]\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        tn = all_samples - tp - fp - fn\n",
    "        cm_classes.append([[tp, fn], [fp, tn]])\n",
    "\n",
    "    return np.asarray(cm_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiQCN9cimfWD"
   },
   "outputs": [],
   "source": [
    "def Report(cm_classes):\n",
    "    \"\"\"\n",
    "    Generate performance metrics report for each class.\n",
    "    \"\"\"\n",
    "    repo = []\n",
    "    for i in range(len(cm_classes)):\n",
    "        tp = cm_classes[i, 0, 0]\n",
    "        fn = cm_classes[i, 0, 1]\n",
    "        fp = cm_classes[i, 1, 0]\n",
    "        tn = cm_classes[i, 1, 1]\n",
    "\n",
    "        iou = tp / (tp + fn + fp)  # Intersection over Union\n",
    "        dsc = (2 * tp) / ((2 * tp) + fp + fn)  # Dice Similarity Coefficient\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn)  # Accuracy\n",
    "        precision = tp / (tp + fp)  # Precision (Positive Predictive Value)\n",
    "        recall = tp / (tp + fn)  # Recall (Sensitivity)\n",
    "        spec = tn / (tn + fp)  # Specificity\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)  # F1-Score\n",
    "\n",
    "        info = {\n",
    "            'IOU': iou,\n",
    "            'DSC': dsc,\n",
    "            'ACC': acc,\n",
    "            'Specificity': spec,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1_score\n",
    "        }\n",
    "\n",
    "        repo.append(info)\n",
    "\n",
    "    return np.asarray(repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J6sEqIdmh1n"
   },
   "outputs": [],
   "source": [
    "def model_performance_report(model, test_dataset, class_labels):\n",
    "    \"\"\"\n",
    "    Generate a performance report for a given model and test data.\n",
    "    \"\"\"\n",
    "    y_pred_all = []\n",
    "    y_test_all = []\n",
    "\n",
    "    for x_test, y_test in test_dataset:\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "        y_pred_all.extend(y_pred_classes)\n",
    "        y_test_all.extend(y_test_classes)\n",
    "\n",
    "    y_pred_all = np.array(y_pred_all)\n",
    "    y_test_all = np.array(y_test_all)\n",
    "\n",
    "    cm = confusion_matrix(y_test_all, y_pred_all)\n",
    "    cm_classes = TFNP(cm)\n",
    "    report = Report(cm_classes)\n",
    "\n",
    "    for i, label in enumerate(class_labels):\n",
    "        print(f\"The report for {label} is : \\n {report[i]} \\n\")\n",
    "\n",
    "    return report, cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVUDlwzUmqpL"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming you have a trained model `image_model`, test dataset `test_dataset`, and class labels `disease_labels`\n",
    "disease_labels = [\"Normal\", \"Covid-19\", \"Viral Pneumonia\", \"Bacterial Pneumonia\", \"Lung Opacity\"]\n",
    "report, cm = model_performance_report(best_model_v2, test_dataset, disease_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKgNPfGXm_2d",
    "outputId": "ab0ea9f5-538c-432b-8a8e-2636f0f18ea6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[288,   6,  10,   4,  19],\n",
       "       [ 10, 315,   0,   0,  12],\n",
       "       [  0,   2, 313,  49,   0],\n",
       "       [  0,   2,  81, 247,   0],\n",
       "       [ 30,  10,   1,   1, 307]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asiRCG_rm9g0",
    "outputId": "2d52c4ac-faa8-4df7-93e1-0f8cfb032ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report for Normal is : \n",
      " {'IOU': 0.784741144414169, 'DSC': 0.8793893129770992, 'ACC': 0.9537199765670767, 'Specificity': 0.9710144927536232, 'Precision': 0.8780487804878049, 'Recall': 0.8807339449541285, 'F1-Score': 0.8793893129770993} \n",
      "\n",
      "The report for Covid-19 is : \n",
      " {'IOU': 0.8823529411764706, 'DSC': 0.9375, 'ACC': 0.9753954305799648, 'Specificity': 0.9854014598540146, 'Precision': 0.9402985074626866, 'Recall': 0.9347181008902077, 'F1-Score': 0.9375} \n",
      "\n",
      "The report for Viral Pneumonia is : \n",
      " {'IOU': 0.6864035087719298, 'DSC': 0.8140442132639792, 'ACC': 0.9162272993555947, 'Specificity': 0.9314966492926284, 'Precision': 0.7728395061728395, 'Recall': 0.8598901098901099, 'F1-Score': 0.8140442132639792} \n",
      "\n",
      "The report for Bacterial Pneumonia is : \n",
      " {'IOU': 0.6432291666666666, 'DSC': 0.7828843106180665, 'ACC': 0.9197422378441711, 'Specificity': 0.9607843137254902, 'Precision': 0.8205980066445183, 'Recall': 0.7484848484848485, 'F1-Score': 0.7828843106180666} \n",
      "\n",
      "The report for Lung Opacity is : \n",
      " {'IOU': 0.8078947368421052, 'DSC': 0.893740902474527, 'ACC': 0.9572349150556532, 'Specificity': 0.9771723122238586, 'Precision': 0.908284023668639, 'Recall': 0.8796561604584527, 'F1-Score': 0.893740902474527} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "disease = {0:\"Normal\", 1:\"Covid-19\", 2:\"Viral Pneumonia\", 3:\"Bacterial Pneumonia\", 4:\"Lung Opacity\"}\n",
    "for i in range(len(report)):\n",
    "  print(f\"The report for {disease[i]} is : \\n {report[i]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
